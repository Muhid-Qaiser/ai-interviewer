{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bcc==0.1.10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting attrs==23.2.0 (from -r requirements.txt (line 1))Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement bcc==0.29.1 (from versions: 0.1.7, 0.1.8, 0.1.10)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for bcc==0.29.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting Babel==2.10.3 (from -r requirements.txt (line 2))\n",
      "  Using cached Babel-2.10.3-py3-none-any.whl.metadata (1.3 kB)\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\D-Program Files\\Python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from transformers import pipeline\n",
    "import wave\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "import sounddevice as sd\n",
    "import os\n",
    "from scipy.io.wavfile import write\n",
    "import openai\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\h'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_3384\\1383357857.py:2: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  PDF_FILE = \"data\\hashir-ayaz-resume.pdf\"\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "PDF_FILE = \"data\\hashir-ayaz-resume.pdf\"\n",
    "\n",
    "# ! Prompt to question till llm has asked 5 questions\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert AI interviewer. You have technical knowledge of whatever domain the user is interviewing for.\n",
    "Your task is to carefully review the experiences and projects provided in the context below, focusing on any technical details, challenges, or skills used.\n",
    "Based on this review, generate insightful, technically focused questions to ask the individual about their experience.\n",
    "You start by introducing the candidate to themselves and then ask a question.\n",
    "Maintaining a conversational tone is important.\n",
    "IMPORTANT POINTS:\n",
    "\n",
    "Make sure the question is related to technical skills required in the Job Position provided below.\n",
    "Focus on the Experiences and Projects section in the Context provided below.\n",
    "Frame the question in a conversational manner that is easy to understand and answer.\n",
    "After a question is answered, you should ask a follow up question to the same question if it is not answered properly or if the answer is not detailed.\n",
    "If the answer is closely related to the job position, you should probe deeper into the answer.\n",
    "\n",
    "IMPORTANT: Only follow up a question, atmost 2 times. If the user does not answer properly even after 2 times, proceed with the next question.\n",
    "Ask a total of 5 question and then conclude the conversation by saying \"Good Bye\" inorder to clarify that the interview has been concluded.\n",
    "\n",
    "\n",
    "Job Position: {field}\n",
    "Context: {context}\n",
    "\n",
    "Return the question as a single string, without introductory phrases like \"Here's a question for you.\"\n",
    "\n",
    "### Example question ### : \"Imagine you're working on a web application with a large dataset of user interactions, and you need to display this data \n",
    "dynamically in the React frontend. Describe how you would design the backend API to support efficient data retrieval and provide a responsive \n",
    "user experience. Specifically, explain how you would manage pagination, filtering, and sorting on the backend using Node.js and MongoDB.\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ! Prompt to question till llm feels it is questioned enough\n",
    "PROMPT_TEMPLATE0 = \"\"\"\n",
    "You are an expert AI interviewer. You have technical knowledge of whatever domain the user is interviewing for.\n",
    "Your task is to carefully review the experiences and projects provided in the context below, focusing on any technical details, challenges, or skills used.\n",
    "Based on this review, generate insightful, technically focused questions to ask the individual about their experience.\n",
    "You start by introducing the candidate to themselves and then ask a question.\n",
    "Maintaining a conversational tone is important.\n",
    "IMPORTANT POINTS:\n",
    "\n",
    "Make sure the question is related to technical skills required in the Job Position provided below.\n",
    "Focus on the Experiences and Projects section in the Context provided below.\n",
    "Frame the question in a conversational manner that is easy to understand and answer.\n",
    "After a question is answered, you should ask a follow up question to the same question if it is not answered properly or if the answer is not detailed.\n",
    "If the answer is closely related to the job position, you should probe deeper into the answer.\n",
    "\n",
    "IMPORTANT: Only follow up a question, atmost 2 times. If the user does not answer properly even after 2 times, proceed with the next question.\n",
    "Ask questions until you feel you have questioned the candidate on a good amount of their data but make sure to not ask any more than 10 questions\n",
    "and then conclude the conversation by saying \"Good Bye\" inorder to clarify that the interview has been concluded.\n",
    "\n",
    "\n",
    "Job Position: {field}\n",
    "Context: {context}\n",
    "\n",
    "Return the question as a single string, without introductory phrases like \"Here's a question for you.\"\n",
    "\n",
    "### Example question ### : \"Imagine you're working on a web application with a large dataset of user interactions, and you need to display this data \n",
    "dynamically in the React frontend. Describe how you would design the backend API to support efficient data retrieval and provide a responsive \n",
    "user experience. Specifically, explain how you would manage pagination, filtering, and sorting on the backend using Node.js and MongoDB.\"\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AI_Interviewer:\n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.tts = None\n",
    "        self.stt = None\n",
    "        self.memory = None\n",
    "  \n",
    "    def create_llm(self, context, field):\n",
    "\n",
    "        llm_used = ChatGroq(\n",
    "            model=\"llama3-8b-8192\",\n",
    "            temperature=0.0,\n",
    "            max_retries=2,\n",
    "        )\n",
    "\n",
    "        prompt = ChatPromptTemplate(\n",
    "            messages=[\n",
    "                # Type your Prompt\n",
    "                SystemMessagePromptTemplate.from_template(\n",
    "                    PROMPT_TEMPLATE.format(context=context, field=field)\n",
    "                ),\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                HumanMessagePromptTemplate.from_template(\"\"\"{text}\"\"\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        memory = ConversationSummaryBufferMemory( llm=llm_used, max_token_limit=3000, memory_key=\"chat_history\", return_messages=True)\n",
    "        conversation = LLMChain(llm=llm_used, prompt=prompt, verbose=False, memory=memory)\n",
    "\n",
    "        return conversation\n",
    "\n",
    "    def initialize_models(self, context, field):\n",
    "\n",
    "        self.llm = self.create_llm(context, field)\n",
    "        self.tts = pipeline(\"text-to-speech\", model=\"facebook/mms-tts-eng\")\n",
    "        self.stt = pipeline(   \"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
    "\n",
    "    def read_csv(self, pdf_file):\n",
    "        return extract_text(pdf_file)\n",
    "\n",
    "    def invoke_llm(self, message):\n",
    "        # Prepare the input for the conversation\n",
    "    \n",
    "        response = self.llm.invoke(message)\n",
    "\n",
    "        question = response['text']\n",
    "        print(\"Interviewer:\")\n",
    "        print(question)\n",
    "\n",
    "        return question\n",
    "\n",
    "    def invoke_tts(self, text, save_audio=False):\n",
    "        audio = self.tts(text)\n",
    "\n",
    "        if not save_audio:\n",
    "            Audio(audio[\"audio\"], rate=16000, autoplay=True)\n",
    "        else:\n",
    "            audio_data = audio[\"audio\"]\n",
    "            audio_data = (audio_data * 32767).astype(np.int16)\n",
    "            sample_rate = 16000\n",
    "            output_file = \"audio.wav\"\n",
    "\n",
    "            with wave.open(output_file, \"wb\") as wf:\n",
    "                wf.setnchannels(1)\n",
    "                wf.setsampwidth(2)\n",
    "                wf.setframerate(sample_rate)\n",
    "                wf.writeframes(audio_data.tobytes())\n",
    "\n",
    "            print(f\"Audio saved to {output_file}\")\n",
    "\n",
    "    def invoke_stt(self, audio_file=\"audio.wav\"):\n",
    "        if not os.path.exists(audio_file) or os.path.getsize(audio_file) == 0:\n",
    "            print(\"Error: audio file does not exist or is empty.\")\n",
    "            return None\n",
    "        transcription = self.stt(audio_file)\n",
    "        return transcription[\"text\"]\n",
    "\n",
    "    def hear_user(self, duration=5, sample_rate=16000):\n",
    "        def record_audio(duration, sample_rate, device=None):\n",
    "            try:\n",
    "                print(f\"Recording for {duration} seconds...\")\n",
    "                audio = sd.rec(\n",
    "                    int(duration * sample_rate),\n",
    "                    samplerate=sample_rate,\n",
    "                    channels=1,\n",
    "                    dtype=np.float32,\n",
    "                    device=device,\n",
    "                )\n",
    "                sd.wait()\n",
    "                print(\"Recording finished!\")\n",
    "                return audio\n",
    "            except Exception as e:\n",
    "                print(\"Error recording audio:\", e)\n",
    "                return None\n",
    "\n",
    "        audio_data = record_audio(duration, sample_rate)\n",
    "        write(\"audio.wav\", sample_rate, audio_data)\n",
    "\n",
    "    def generate_report(self, interview_responses, job):\n",
    "        REPORT_TEMPLATE = \"\"\"\n",
    "        You are a recruiter and you want to hire the best candidate for the job position provided below.\n",
    "        You are being provided with a list of questions and answers from an interview. Your task is to generate a report based on the interview responses. You have expertise in the field of the job position provided below. you will give the employee a rating out of 10 and a list of strengths and weaknesses.\n",
    "        Job Position: {job}\n",
    "        Interview Responses: {interview_responses}\n",
    "        \n",
    "        The report should be in the following format:\n",
    "        {\n",
    "            strengths: [list of strengths],\n",
    "            weaknesses: [list of weaknesses],\n",
    "            overall_rating: [rating out of 10],\n",
    "            overall_feedback: [feedback],\n",
    "        }\n",
    "        \"\"\"\n",
    "        prompt_template = ChatPromptTemplate.from_template(REPORT_TEMPLATE)\n",
    "        pass\n",
    "\n",
    "    def start_interview(self, job):\n",
    "\n",
    "        # Initialize interview variables and context\n",
    "        interview_responses = []\n",
    "\n",
    "        # Loop until the interview is complete\n",
    "        while True:\n",
    "\n",
    "            # Generate question using chat history\n",
    "            current_question = self.invoke_llm(\"Hello\")\n",
    "\n",
    "            print(f\"\\nInterview Question: {current_question}\")\n",
    "\n",
    "            if \"good bye\" in current_question.lower():\n",
    "                print(\"Ending interview based on user request.\")\n",
    "                break\n",
    "\n",
    "            # Record and transcribe user's response\n",
    "            print(\"\\nRecording User's Answer:\")\n",
    "            self.hear_user(duration=20, sample_rate=16000)\n",
    "            user_response = self.invoke_stt(\"audio.wav\")\n",
    "            print(\"\\nUser Response:\", user_response)\n",
    "\n",
    "            # Append to interview responses\n",
    "            interview_responses.append(\n",
    "                {\"question\": current_question, \"answer\": user_response}\n",
    "            )\n",
    "\n",
    "            if \"stop\" in user_response.lower() :\n",
    "                print(\"Ending interview based on user request.\")\n",
    "                break\n",
    "\n",
    "\n",
    "        # Generate final report\n",
    "        interview_summary = self.generate_report(interview_responses, job)\n",
    "\n",
    "        print(\"\\nInterview Summary:\")\n",
    "        print(interview_summary)\n",
    "\n",
    "        return interview_responses, interview_summary\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/mms-tts-eng were not used when initializing VitsModel: ['flow.flows.0.wavenet.in_layers.0.weight_g', 'flow.flows.0.wavenet.in_layers.0.weight_v', 'flow.flows.0.wavenet.in_layers.1.weight_g', 'flow.flows.0.wavenet.in_layers.1.weight_v', 'flow.flows.0.wavenet.in_layers.2.weight_g', 'flow.flows.0.wavenet.in_layers.2.weight_v', 'flow.flows.0.wavenet.in_layers.3.weight_g', 'flow.flows.0.wavenet.in_layers.3.weight_v', 'flow.flows.0.wavenet.res_skip_layers.0.weight_g', 'flow.flows.0.wavenet.res_skip_layers.0.weight_v', 'flow.flows.0.wavenet.res_skip_layers.1.weight_g', 'flow.flows.0.wavenet.res_skip_layers.1.weight_v', 'flow.flows.0.wavenet.res_skip_layers.2.weight_g', 'flow.flows.0.wavenet.res_skip_layers.2.weight_v', 'flow.flows.0.wavenet.res_skip_layers.3.weight_g', 'flow.flows.0.wavenet.res_skip_layers.3.weight_v', 'flow.flows.1.wavenet.in_layers.0.weight_g', 'flow.flows.1.wavenet.in_layers.0.weight_v', 'flow.flows.1.wavenet.in_layers.1.weight_g', 'flow.flows.1.wavenet.in_layers.1.weight_v', 'flow.flows.1.wavenet.in_layers.2.weight_g', 'flow.flows.1.wavenet.in_layers.2.weight_v', 'flow.flows.1.wavenet.in_layers.3.weight_g', 'flow.flows.1.wavenet.in_layers.3.weight_v', 'flow.flows.1.wavenet.res_skip_layers.0.weight_g', 'flow.flows.1.wavenet.res_skip_layers.0.weight_v', 'flow.flows.1.wavenet.res_skip_layers.1.weight_g', 'flow.flows.1.wavenet.res_skip_layers.1.weight_v', 'flow.flows.1.wavenet.res_skip_layers.2.weight_g', 'flow.flows.1.wavenet.res_skip_layers.2.weight_v', 'flow.flows.1.wavenet.res_skip_layers.3.weight_g', 'flow.flows.1.wavenet.res_skip_layers.3.weight_v', 'flow.flows.2.wavenet.in_layers.0.weight_g', 'flow.flows.2.wavenet.in_layers.0.weight_v', 'flow.flows.2.wavenet.in_layers.1.weight_g', 'flow.flows.2.wavenet.in_layers.1.weight_v', 'flow.flows.2.wavenet.in_layers.2.weight_g', 'flow.flows.2.wavenet.in_layers.2.weight_v', 'flow.flows.2.wavenet.in_layers.3.weight_g', 'flow.flows.2.wavenet.in_layers.3.weight_v', 'flow.flows.2.wavenet.res_skip_layers.0.weight_g', 'flow.flows.2.wavenet.res_skip_layers.0.weight_v', 'flow.flows.2.wavenet.res_skip_layers.1.weight_g', 'flow.flows.2.wavenet.res_skip_layers.1.weight_v', 'flow.flows.2.wavenet.res_skip_layers.2.weight_g', 'flow.flows.2.wavenet.res_skip_layers.2.weight_v', 'flow.flows.2.wavenet.res_skip_layers.3.weight_g', 'flow.flows.2.wavenet.res_skip_layers.3.weight_v', 'flow.flows.3.wavenet.in_layers.0.weight_g', 'flow.flows.3.wavenet.in_layers.0.weight_v', 'flow.flows.3.wavenet.in_layers.1.weight_g', 'flow.flows.3.wavenet.in_layers.1.weight_v', 'flow.flows.3.wavenet.in_layers.2.weight_g', 'flow.flows.3.wavenet.in_layers.2.weight_v', 'flow.flows.3.wavenet.in_layers.3.weight_g', 'flow.flows.3.wavenet.in_layers.3.weight_v', 'flow.flows.3.wavenet.res_skip_layers.0.weight_g', 'flow.flows.3.wavenet.res_skip_layers.0.weight_v', 'flow.flows.3.wavenet.res_skip_layers.1.weight_g', 'flow.flows.3.wavenet.res_skip_layers.1.weight_v', 'flow.flows.3.wavenet.res_skip_layers.2.weight_g', 'flow.flows.3.wavenet.res_skip_layers.2.weight_v', 'flow.flows.3.wavenet.res_skip_layers.3.weight_g', 'flow.flows.3.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.0.weight_g', 'posterior_encoder.wavenet.in_layers.0.weight_v', 'posterior_encoder.wavenet.in_layers.1.weight_g', 'posterior_encoder.wavenet.in_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.10.weight_g', 'posterior_encoder.wavenet.in_layers.10.weight_v', 'posterior_encoder.wavenet.in_layers.11.weight_g', 'posterior_encoder.wavenet.in_layers.11.weight_v', 'posterior_encoder.wavenet.in_layers.12.weight_g', 'posterior_encoder.wavenet.in_layers.12.weight_v', 'posterior_encoder.wavenet.in_layers.13.weight_g', 'posterior_encoder.wavenet.in_layers.13.weight_v', 'posterior_encoder.wavenet.in_layers.14.weight_g', 'posterior_encoder.wavenet.in_layers.14.weight_v', 'posterior_encoder.wavenet.in_layers.15.weight_g', 'posterior_encoder.wavenet.in_layers.15.weight_v', 'posterior_encoder.wavenet.in_layers.2.weight_g', 'posterior_encoder.wavenet.in_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.3.weight_g', 'posterior_encoder.wavenet.in_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.4.weight_g', 'posterior_encoder.wavenet.in_layers.4.weight_v', 'posterior_encoder.wavenet.in_layers.5.weight_g', 'posterior_encoder.wavenet.in_layers.5.weight_v', 'posterior_encoder.wavenet.in_layers.6.weight_g', 'posterior_encoder.wavenet.in_layers.6.weight_v', 'posterior_encoder.wavenet.in_layers.7.weight_g', 'posterior_encoder.wavenet.in_layers.7.weight_v', 'posterior_encoder.wavenet.in_layers.8.weight_g', 'posterior_encoder.wavenet.in_layers.8.weight_v', 'posterior_encoder.wavenet.in_layers.9.weight_g', 'posterior_encoder.wavenet.in_layers.9.weight_v', 'posterior_encoder.wavenet.res_skip_layers.0.weight_g', 'posterior_encoder.wavenet.res_skip_layers.0.weight_v', 'posterior_encoder.wavenet.res_skip_layers.1.weight_g', 'posterior_encoder.wavenet.res_skip_layers.1.weight_v', 'posterior_encoder.wavenet.res_skip_layers.10.weight_g', 'posterior_encoder.wavenet.res_skip_layers.10.weight_v', 'posterior_encoder.wavenet.res_skip_layers.11.weight_g', 'posterior_encoder.wavenet.res_skip_layers.11.weight_v', 'posterior_encoder.wavenet.res_skip_layers.12.weight_g', 'posterior_encoder.wavenet.res_skip_layers.12.weight_v', 'posterior_encoder.wavenet.res_skip_layers.13.weight_g', 'posterior_encoder.wavenet.res_skip_layers.13.weight_v', 'posterior_encoder.wavenet.res_skip_layers.14.weight_g', 'posterior_encoder.wavenet.res_skip_layers.14.weight_v', 'posterior_encoder.wavenet.res_skip_layers.15.weight_g', 'posterior_encoder.wavenet.res_skip_layers.15.weight_v', 'posterior_encoder.wavenet.res_skip_layers.2.weight_g', 'posterior_encoder.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.res_skip_layers.3.weight_g', 'posterior_encoder.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.res_skip_layers.4.weight_g', 'posterior_encoder.wavenet.res_skip_layers.4.weight_v', 'posterior_encoder.wavenet.res_skip_layers.5.weight_g', 'posterior_encoder.wavenet.res_skip_layers.5.weight_v', 'posterior_encoder.wavenet.res_skip_layers.6.weight_g', 'posterior_encoder.wavenet.res_skip_layers.6.weight_v', 'posterior_encoder.wavenet.res_skip_layers.7.weight_g', 'posterior_encoder.wavenet.res_skip_layers.7.weight_v', 'posterior_encoder.wavenet.res_skip_layers.8.weight_g', 'posterior_encoder.wavenet.res_skip_layers.8.weight_v', 'posterior_encoder.wavenet.res_skip_layers.9.weight_g', 'posterior_encoder.wavenet.res_skip_layers.9.weight_v']\n",
      "- This IS expected if you are initializing VitsModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VitsModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VitsModel were not initialized from the model checkpoint at facebook/mms-tts-eng and are newly initialized: ['flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "interviewer = AI_Interviewer()\n",
    "cv_description = interviewer.read_csv(PDF_FILE)\n",
    "interviewer.initialize_models(cv_description, \"MERN developer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interviewer:\n",
      "Hello Hashir! It's great to have you here today. I'm excited to learn more about your experience and skills as a MERN developer. Let's dive right in!\n",
      "\n",
      "Can you tell me more about your experience with Docker and GitHub Actions in your current role as a Backend Engineering Intern at Cosmosys? How did you set up the continuous integration and deployment pipelines, and what kind of benefits did you see from implementing this process?\n",
      "\n",
      "Interview Question: Hello Hashir! It's great to have you here today. I'm excited to learn more about your experience and skills as a MERN developer. Let's dive right in!\n",
      "\n",
      "Can you tell me more about your experience with Docker and GitHub Actions in your current role as a Backend Engineering Intern at Cosmosys? How did you set up the continuous integration and deployment pipelines, and what kind of benefits did you see from implementing this process?\n",
      "\n",
      "Recording User's Answer:\n",
      "Recording for 20 seconds...\n",
      "Recording finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\D-Program Files\\Python\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Response:  Yes, I did work on my project as a backend engineer. It was capable but I learned a lot of new things and a lot of new pipelines such as CIDI or CD pipelines and they helped me understand better\n",
      "Interviewer:\n",
      "Nice to meet you again, Hashir! So, you mentioned setting up a continuous integration and deployment pipeline using Docker and GitHub Actions, which reduced deployment time by 40%. That's impressive!\n",
      "\n",
      "To drill down a bit further, can you walk me through the specific steps you took to set up the pipeline, and how exactly did you measure the 40% reduction in deployment time?\n",
      "\n",
      "Interview Question: Nice to meet you again, Hashir! So, you mentioned setting up a continuous integration and deployment pipeline using Docker and GitHub Actions, which reduced deployment time by 40%. That's impressive!\n",
      "\n",
      "To drill down a bit further, can you walk me through the specific steps you took to set up the pipeline, and how exactly did you measure the 40% reduction in deployment time?\n",
      "\n",
      "Recording User's Answer:\n",
      "Recording for 20 seconds...\n",
      "Recording finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\D-Program Files\\Python\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Response:  I used were CI or CD pipeline and I also worked on MongoDB to integrate them and yes I did measure\n",
      "Interviewer:\n",
      "I think we might be getting a little stuck here, Hashir! It seems like we're not quite connecting on this question. To help move forward, can you try to provide a more detailed answer about the steps you took to set up the pipeline and how you measured the 40% reduction in deployment time?\n",
      "\n",
      "Interview Question: I think we might be getting a little stuck here, Hashir! It seems like we're not quite connecting on this question. To help move forward, can you try to provide a more detailed answer about the steps you took to set up the pipeline and how you measured the 40% reduction in deployment time?\n",
      "\n",
      "Recording User's Answer:\n",
      "Recording for 20 seconds...\n",
      "Recording finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\D-Program Files\\Python\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Response:  I basically used CIDA pipelines and how did I measure the 40% reduction? I basically used\n",
      "Interviewer:\n",
      "I think we've reached a bit of a roadblock, Hashir. It seems like we're not quite getting the information we need to move forward. Let's try to pivot a bit and explore another area of your experience.\n",
      "\n",
      "Can you tell me more about your work on the Hostel Management Site project, specifically how you applied MVC architecture principles to create a scalable and maintainable codebase? What were some of the key challenges you faced, and how did you overcome them?\n",
      "\n",
      "Interview Question: I think we've reached a bit of a roadblock, Hashir. It seems like we're not quite getting the information we need to move forward. Let's try to pivot a bit and explore another area of your experience.\n",
      "\n",
      "Can you tell me more about your work on the Hostel Management Site project, specifically how you applied MVC architecture principles to create a scalable and maintainable codebase? What were some of the key challenges you faced, and how did you overcome them?\n",
      "\n",
      "Recording User's Answer:\n",
      "Recording for 20 seconds...\n",
      "Recording finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\D-Program Files\\Python\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Response:  With the Hodo Management app, I use MVCR architecture because it is very scalable and maintainable in the code base. Users can easily enter themselves and we can easily scale them to many more users and I use\n",
      "Interviewer:\n",
      "I think we've reached the end of our conversation, Hashir. It's been great speaking with you, but I'm not getting the level of detail I need to further explore your experience and skills.\n",
      "\n",
      "It was a pleasure talking to you, and I hope our conversation has been helpful in some way. Good Bye!\n",
      "\n",
      "Interview Question: I think we've reached the end of our conversation, Hashir. It's been great speaking with you, but I'm not getting the level of detail I need to further explore your experience and skills.\n",
      "\n",
      "It was a pleasure talking to you, and I hope our conversation has been helpful in some way. Good Bye!\n",
      "\n",
      "Recording User's Answer:\n",
      "Recording for 20 seconds...\n",
      "Recording finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\D-Program Files\\Python\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Response:  Goodbye.\n",
      "Ending interview based on user request.\n",
      "\n",
      "Interview Summary:\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'question': \"Hello Hashir! It's great to have you here today. I'm excited to learn more about your experience and skills as a MERN developer. Let's dive right in!\\n\\nCan you tell me more about your experience with Docker and GitHub Actions in your current role as a Backend Engineering Intern at Cosmosys? How did you set up the continuous integration and deployment pipelines, and what kind of benefits did you see from implementing this process?\",\n",
       "   'answer': ' Yes, I did work on my project as a backend engineer. It was capable but I learned a lot of new things and a lot of new pipelines such as CIDI or CD pipelines and they helped me understand better'},\n",
       "  {'question': \"Nice to meet you again, Hashir! So, you mentioned setting up a continuous integration and deployment pipeline using Docker and GitHub Actions, which reduced deployment time by 40%. That's impressive!\\n\\nTo drill down a bit further, can you walk me through the specific steps you took to set up the pipeline, and how exactly did you measure the 40% reduction in deployment time?\",\n",
       "   'answer': ' I used were CI or CD pipeline and I also worked on MongoDB to integrate them and yes I did measure'},\n",
       "  {'question': \"I think we might be getting a little stuck here, Hashir! It seems like we're not quite connecting on this question. To help move forward, can you try to provide a more detailed answer about the steps you took to set up the pipeline and how you measured the 40% reduction in deployment time?\",\n",
       "   'answer': ' I basically used CIDA pipelines and how did I measure the 40% reduction? I basically used'},\n",
       "  {'question': \"I think we've reached a bit of a roadblock, Hashir. It seems like we're not quite getting the information we need to move forward. Let's try to pivot a bit and explore another area of your experience.\\n\\nCan you tell me more about your work on the Hostel Management Site project, specifically how you applied MVC architecture principles to create a scalable and maintainable codebase? What were some of the key challenges you faced, and how did you overcome them?\",\n",
       "   'answer': ' With the Hodo Management app, I use MVCR architecture because it is very scalable and maintainable in the code base. Users can easily enter themselves and we can easily scale them to many more users and I use'},\n",
       "  {'question': \"I think we've reached the end of our conversation, Hashir. It's been great speaking with you, but I'm not getting the level of detail I need to further explore your experience and skills.\\n\\nIt was a pleasure talking to you, and I hope our conversation has been helpful in some way. Good Bye!\",\n",
       "   'answer': ' Goodbye.'}],\n",
       " None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interviewer.start_interview(\n",
    "    job=\"Mern stack developer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
